% Bibliographie der Seminararbeit Big Data im Wintersemester 2018/2019
    
@article{Jaeger2003,
author = {Jaeger, Herbert},
year = {2003},
month = {06},
pages = {},
title = {Adaptive Nonlinear System Identification with Echo State Networks},
journal = {NIPS}
}

@article{YILDIZ20121,
title = {Re-visiting the echo state property},
journal = {Neural Networks},
volume = {35},
pages = {1 - 9},
year = {2012},
issn = {0893-6080},
url = {http://www.sciencedirect.com/science/article/pii/S0893608012001852},
author = {Izzet B. Yildiz and Herbert Jaeger and Stefan J. Kiebel}
}


@Inbook{Lukosecicius2012,
author="Luko{\v{s}}evi{\v{c}}ius, Mantas",
title="A Practical Guide to Applying Echo State Networks",
bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="659--686",
isbn="978-3-642-35289-8",
doi="10.1007/978-3-642-35289-8_36",
url="https://doi.org/10.1007/978-3-642-35289-8_36"
}


@misc{Wu2018StatChallenges,
author = {Wu, Qiuyi and Fokoué, Ernest and Kudithipudi, Dhireesha},
year = {2018},
month = {2},
pages = {},
title = {On the Statistical Challenges of Echo State Networks and Some Potential Remedies},
url = {https://arxiv.org/abs/1802.07369}
}

@article {Jaeger2004Harness,
	author = {Jaeger, Herbert and Haas, Harald},
	title = {Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication},
	volume = {304},
	number = {5667},
	pages = {78--80},
	year = {2004},
	doi = {10.1126/science.1091277},
	publisher = {American Association for the Advancement of Science},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/304/5667/78},
	eprint = {http://science.sciencemag.org/content/304/5667/78.full.pdf},
	journal = {Science}
}

@article{Jaeger2007Scholarpeia,
AUTHOR = {Jaeger, H. },
TITLE   = {{k}cho state network},
YEAR    = {2007},
JOURNAL = {Scholarpedia},
VOLUME  = {2},
NUMBER  = {9},
PAGES   = {2330},
DOI     = {10.4249/scholarpedia.2330},
NOTE    = {revision \#188245}
}

@inproceedings{Goudarzi2015MultiplNeuron,
author = {Goudarzi, Alireza and Shabani, Alireza and Stefanovic, Darko},
year = {2015},
month = {07},
pages = {1-8},
title = {Product reservoir computing: Time-series computation with multiplicative neurons},
doi = {10.1109/IJCNN.2015.7280453}
}

@article{Deng2007CollectiveBO,
  title={Collective Behavior of a Small-World Recurrent Neural System With Scale-Free Distribution},
  author={Zhidong Deng and Yi Zhang},
  journal={IEEE Transactions on Neural Networks},
  year={2007},
  volume={18},
  pages={1364-1375}
}

@article{Qingsong2010ConnStruct,
author = {Song, Qingsong and Feng, Zuren},
year = {2010},
month = {06},
pages = {2177-2185},
title = {Effects of connectivity structure of complex echo state network on its prediction performance for nonlinear time series},
volume = {73},
journal = {Neurocomputing},
doi = {10.1016/j.neucom.2010.01.015}
}

@article{StanekCommitteeMethods,
author = {Stanek, Konrad},
title = {Reservoir computing in financial forecasting using committee methods},
year = {2011},
publisher = {Technical University of Denmark},
journal = {Master Thesis}
}

@article{MackeyGlass2010SCholar,
AUTHOR = {Glass, L.  and Mackey, M. },
TITLE   = {{M}ackey-{G}lass equation},
YEAR    = {2010},
JOURNAL = {Scholarpedia},
VOLUME  = {5},
NUMBER  = {3},
PAGES   = {6908},
DOI     = {10.4249/scholarpedia.6908},
NOTE    = {revision \#186443}
}

@article{Gallichio2017DeepRC,
title = {Deep reservoir computing: A critical experimental analysis},
journal = {Neurocomputing},
volume = {268},
pages = {87 - 99},
year = {2017},
note = {Advances in artificial neural networks, machine learning and computational intelligence},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.12.089},
url = {http://www.sciencedirect.com/science/article/pii/S0925231217307567},
author = {Claudio Gallicchio and Alessio Micheli and Luca Pedrelli},
keywords = {Reservoir computing, Echo State Networks, Deep Learning, Deep neural networks, Recurrent neural networks, Multiple time-scale dynamics},
}

@misc{Gallichio2017DeepRCSurvey,
author = {Gallicchio, Claudio and Micheli, Alessio},
year = {2017},
month = {12},
pages = {},
title = {Deep Echo State Network (DeepESN): A Brief Survey},
archivePrefix={arXiv}
}

@article{Jaeger2001,
author = {Jaeger, Herbert},
year = {2001},
month = {1},
pages = {},
title = {The echo state approach to analysing and training recurrent neural networks-with an erratum note},
volume = {148},
journal = {Bonn, Germany: German National Research Center for Information Technology GMD Technical Report}
}

@article{Grigortega2018SAS,
author = {Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
year = {2018},
month = {09},
pages = {1-40},
title = {Universal discrete-time reservoir computers with stochastic inputs and linear readouts using non-homogeneous state-affine systems},
volume = {19},
journal = {Journal of Machine Learning Research}
}

@article{Grigoryeva2014TimeDelayRC,
title = {Stochastic nonlinear time series forecasting using time-delay reservoir computers: Performance and universality},
journal = {Neural Networks},
volume = {55},
pages = {59 - 719},
year = {2014},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.03.004},
url = {http://www.sciencedirect.com/science/article/pii/S0893608014000586},
author = {Lyudmila Grigoryeva and Julie Henriques and Laurent Larger and Juan-Pablo Ortega}
}

@article{Grigortega2018Univeral,
title = {Echo state networks are universal},
journal = {Neural Networks},
volume = {108},
pages = {495 - 508},
year = {2018},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.08.025},
url = {http://www.sciencedirect.com/science/article/pii/S089360801830251X},
author = {Lyudmila Grigoryeva and Juan-Pablo Ortega}
}

@misc{Ortega2018UniversalityStochInput,
author = {Gonon, Lukas and Ortega, Juan-Pablo},
year = {2018},
month = {7},
pages = {},
title = {Reservoir Computing Universality With Stochastic Inputs},
note = {Preprint},
archivePrefix={arXiv}
}

@article{Grigortega2016Ridge,
author = {Grigoryeva, Lyudmila and Ortega, Juan-Pablo},
year = {2016},
month = {05},
pages = {},
title = {Singular Ridge Regression with Homoscedastic Residuals: Generalization Error with Estimated Parameters},
journal = {SSRN Electronic Journal},
doi = {10.2139/ssrn.2782670}
}

@article{Maass2002LSM,
author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
year = {2002},
month = {12},
pages = {2531-60},
title = {Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations},
volume = {14},
journal = {Neural computation},
doi = {10.1162/089976602760407955}
}

@inproceedings{Schrauwen2007Overview,
  author       = {Schrauwen, Benjamin and Verstraeten, David and Van Campenhout, Jan},
  booktitle    = {Proceedings of the 15th European Symposium on Artificial Neural Networks},
  pages        = {471--482},
  title        = {An overview of reservoir computing: theory, applications and implementations},
  url          = {http://dx.doi.org/1854/11063},
  year         = {2007}
}

@inproceedings{Maat2018EfficientOptimization,
author = {Maat, Jacob Reinier and Gianniotis, Nikos and Protopapas, Pavlos},
year = {2018},
month = {07},
pages = {1-7},
title = {Efficient Optimization of Echo State Networks for Time Series Datasets},
doi = {10.1109/IJCNN.2018.8489094}
}


@article{Huang2006ELM,
title = {Extreme learning machine: Theory and applications},
journal = {Neurocomputing},
volume = {70},
number = {1},
pages = {489 - 501},
year = {2006},
note = {Neural Networks},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2005.12.126},
url = {http://www.sciencedirect.com/science/article/pii/S0925231206000385},
author = {Guang-Bin Huang and Qin-Yu Zhu and Chee-Kheong Siew}
}

@article{Crook2007NonLinTrans,
title = {Nonlinear transient computation},
journal = {Neurocomputing},
volume = {70},
number = {7},
pages = {1167 - 1176},
year = {2007},
note = {Advances in Computational Intelligence and Learning},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2006.10.148},
url = {http://www.sciencedirect.com/science/article/pii/S0925231206004693},
author = {Nigel Crook}
}

@article{Buonomano1995TemporalIT,
  title={Temporal information transformed into a spatial code by a neural network with realistic properties},
  author={Dean V. Buonomano and Michael Merzenich},
  journal={Science},
  year={1995},
  volume={267 5200},
  pages={1028-1030}
}

@article{BasicESNStructure,
AUTHOR = {Xu, Xiaomin and Niu, Dongxiao and Fu, Ming and Xia, Huicong and Wu, Han},
TITLE = {A Multi Time Scale Wind Power Forecasting Model of a Chaotic Echo State Network Based on a Hybrid Algorithm of Particle Swarm Optimization and Tabu Search},
JOURNAL = {Energies},
VOLUME = {8},
YEAR = {2015},
NUMBER = {11},
PAGES = {12388--12408},
URL = {http://www.mdpi.com/1996-1073/8/11/12317},
ISSN = {1996-1073}
}

   
@article{Schrauwen2008,
  title    = {Improving reservoirs using intrinsic plasticity},
  journal  = {Neurocomputing},
  volume   = {71},
  number   = {7},
  pages    = {1159 - 1171},
  year     = {2008},
  note     = {Progress in Modeling, Theory, and Application of Computational Intelligence},
  issn     = {0925-2312},
  doi      = {https://doi.org/10.1016/j.neucom.2007.12.020},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925231208000519},
  author   = {Benjamin Schrauwen and Marion Wardermann and David Verstraeten and Jochen J. Steil and Dirk Stroobandt},
  keywords = {Reservoir computing, Intrinsic plasticity, Information maximization},
  abstract = {The benefits of using intrinsic plasticity (IP), an unsupervised, local, biologically inspired adaptation rule that tunes the probability density of a neuron's output towards an exponential distribution—thereby realizing an information maximization—have already been demonstrated. In this work, we extend the ideas of this adaptation method to a more commonly used non-linearity and a Gaussian output distribution. After deriving the learning rules, we show the effects of the bounded output of the transfer function on the moments of the actual output distribution. This allows us to show that the rule converges to the expected distributions, even in random recurrent networks. The IP rule is evaluated in a reservoir computing setting, which is a temporal processing technique which uses random, untrained recurrent networks as excitable media, where the network's state is fed to a linear regressor used to calculate the desired output. We present an experimental comparison of the different IP rules on three benchmark tasks with different characteristics. Furthermore, we show that this unsupervised reservoir adaptation is able to adapt networks with very constrained topologies, such as a 1D lattice which generally shows quite unsuitable dynamic behavior, to a reservoir that can be used to solve complex tasks. We clearly demonstrate that IP is able to make reservoir computing more robust: the internal dynamics can autonomously tune themselves—irrespective of initial weights or input scaling—to the dynamic regime which is optimal for a given task.}
}


@article{Xiaomin2015WindPower,
title = {A Multi Time Scale Wind Power Forecasting Model of a Chaotic Echo State Network Based on a Hybrid Algorithm of Particle Swarm Optimization and Tabu Search},
author = {Xu, Xiaomin and Niu, Dongxiao and Fu, Ming and Xia, Huicong and Wu, Han},
year = {2015},
journal = {Energies},
volume = {8},
number = {11},
pages = {1-21},
url = {https://EconPapers.repec.org/RePEc:gam:jeners:v:8:y:2015:i:11:p:12317-12408:d:58212}
}


@misc{basterrech2015experimental,
    title={An Experimental Analysis of the Echo State Network Initialization Using the Particle Swarm Optimization},
    author={Sebastián Basterrech and Enrique Alba and Václav Snášel},
    year={2015},
    eprint={1501.00436},
    archivePrefix={arXiv},
    primaryClass={cs.NE}
}

@article{Verzelli2019,
   title={Echo State Networks with Self-Normalizing Activations on the Hyper-Sphere},
   volume={9},
   ISSN={2045-2322},
   url={http://dx.doi.org/10.1038/s41598-019-50158-4},
   DOI={10.1038/s41598-019-50158-4},
   number={1},
   journal={Scientific Reports},
   publisher={Springer Science and Business Media LLC},
   author={Verzelli, Pietro and Alippi, Cesare and Livi, Lorenzo},
   year={2019},
   month={Sep}
}

@article{Boedecker2009SelfOrganized,
author = { Joschka   Boedecker  and  Oliver   Obst  and  N. Michael   Mayer  and  Minoru   Asada },
title = {Initialization and self‐organized optimization of recurrent neural network connectivity},
journal = {HFSP Journal},
volume = {3},
number = {5},
pages = {340-349},
year  = {2009},
publisher = {Taylor & Francis},
doi = {10.2976/1.3240502},
note ={PMID: 20357891},
URL = {https://doi.org/10.2976/1.3240502},
eprint = {https://doi.org/10.2976/1.3240502}

}


@InProceedings{Triesch2005,
author={Triesch, Jochen},
title={A Gradient Rule for the Plasticity of a Neuron's Intrinsic Excitability},
booktitle={Artificial Neural Networks: Biological Inspirations -- ICANN 2005},
year={2005},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={65--70},
isbn={978-3-540-28754-4}
}


@article{Strauss2012Design,
author = {Strauß, Tobias and Wustlich, Welf and Labahn, Roger},
year = {2012},
month = {09},
pages = {},
title = {Design Strategies for Weight Matrices of Echo State Networks},
volume = {24},
journal = {Neural computation},
doi = {10.1162/NECO_a_00374}
}


@misc{gormley2018mixtures,
    title={Mixtures of Experts Models},
    author={Isobel Claire Gormley and Sylvia Frühwirth-Schnatter},
    year={2018},
    eprint={1806.08200},
    archivePrefix={arXiv},
    primaryClass={stat.ME}
}


@article{Adamskiy2016AdaptiveRegret,
  author  = {Dmitry Adamskiy and Wouter M. Koolen and Alexey Chernov and Vladimir Vovk},
  title   = {A Closer Look at Adaptive Regret},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {23},
  pages   = {1-21},
  url     = {http://jmlr.org/papers/v17/13-533.html}
}

@book{CesaBianchi2006PredictionLearningGames, 
author = {Cesa-Bianchi, Nicolo and Lugosi, Gabor}, 
title = {Prediction, Learning, and Games}, 
year = {2006}, 
isbn = {0521841089}, 
publisher = {Cambridge University Press}, 
address = {USA} 
}


@article{MaChenLi2019ConvESN, 
author={Q. Ma and E. Chen and Z. Lin and J. Yan and Z. Yu and W. W. Y. Ng}, 
journal={IEEE Transactions on Cybernetics}, 
title={Convolutional Multitimescale Echo State Network}, 
year={2019}, 
volume={}, 
number={}, 
pages={1-13}, 
doi={10.1109/TCYB.2019.2919648}, 
ISSN={2168-2275}
}

@article{Kalliovirta2015GMUnivariateSeries,
author = {Kalliovirta, Leena and Meitz, Mika and Saikkonen, Pentti},
title = {A Gaussian Mixture Autoregressive Model for Univariate Time Series},
journal = {Journal of Time Series Analysis},
volume = {36},
number = {2},
pages = {247-266},
keywords = {ergodicity, Markov chain, nonlinear autoregression, regime switching, stationarity},
doi = {10.1111/jtsa.12108},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jtsa.12108},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/jtsa.12108},
abstract = {The Gaussian mixture autoregressive model studied in this article belongs to the family of mixture autoregressive models, but it differs from its previous alternatives in several advantageous ways. A major theoretical advantage is that, by the definition of the model, conditions for stationarity and ergodicity are always met and these properties are much more straightforward to establish than is common in nonlinear autoregressive models. Another major advantage is that, for a pth-order model, explicit expressions of the stationary distributions of dimension p + 1 or smaller are known and given by mixtures of Gaussian distributions with constant mixing weights. In contrast, the conditional distribution given the past observations is a Gaussian mixture with time-varying mixing weights that depend on p lagged values of the series in a natural and parsimonious way. Because of the known stationary distribution, exact maximum likelihood estimation is feasible and one can assess the applicability of the model in advance by using a non-parametric estimate of the stationary density. An empirical example with interest rate series illustrates the practical usefulness and flexibility of the model, particularly in allowing for level shifts and temporary changes in variance. Copyright © 2014 Wiley Publishing Ltd},
year = {2015}
}


@InProceedings{Koprinkova2011IPandStability,
author="Koprinkova-Hristova, Petia
and Palm, Guenther",
editor="Honkela, Timo
and Duch, W{\l}odzis{\l}aw
and Girolami, Mark
and Kaski, Samuel",
title="ESN Intrinsic Plasticity versus Reservoir Stability",
booktitle="Artificial Neural Networks and Machine Learning -- ICANN 2011",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="69--76",
abstract="The work presented in this paper was inspired by similarities between intrinsic plasticity (IP) pre-training of the ESN reservoir and the common RNN stability conditions derived from nonlinear control theory. The common theoretical stability conditions were applied to the ESN structure. It was proven that in fact IP training achieves a balance between maximization of entropy at the ESN output and the concentration of that output distribution around the pre-specified mean value. Thus the squeezing of the neuron nonlinearities is produced not only by nonzero biases and translation of the ESN equilibrium state but also by the chosen output distribution mean value. The numerical investigations of different random reservoirs showed that the IP improvement stabilizes even initially unstable reservoirs.",
isbn="978-3-642-21735-7"
}


@InProceedings{Engel2010IncrementalGaussianMixtures,
author="Engel, Paulo Martins
and Heinen, Milton Roberto",
editor="da Rocha Costa, Ant{\^o}nio Carlos
and Vicari, Rosa Maria
and Tonidandel, Flavio",
title="Incremental Learning of Multivariate Gaussian Mixture Models",
booktitle="Advances in Artificial Intelligence -- SBIA 2010",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="82--91",
abstract="This paper presents a new algorithm for unsupervised incremental learning based on a Bayesian framework. The algorithm, called IGMM (for Incremental Gaussian Mixture Model), creates and continually adjusts a Gaussian Mixture Model consistent to all sequentially presented data. IGMM is particularly useful for on-line incremental clustering of data streams, as encountered in the domain of mobile robotics and animats. It creates an incremental knowledge model of the domain consisting of primitive concepts involving all observed variables. We present some preliminary results obtained using synthetic data and also consider practical issues as convergence properties discuss future developments.",
isbn="978-3-642-16138-4"
}


@inproceedings{Heinen2011IGMN,
author = {Heinen, Milton and Engel, Paulo and Pinto, Rafael},
year = {2011},
month = {01},
pages = {},
title = {IGMN: An Incremental Gaussian Mixture Network that Learns Instantaneously from Data Flows}
}


@inproceedings{ESIGM2011,
author = {Pinto, Rafael and Engel, Paulo and Heinen, Milton},
year = {2011},
month = {01},
pages = {},
title = {Echo State Incremental Gaussian Mixture Network for Spatio-Temporal Pattern Processing},
doi = {10.21528/CBIC2011-32.4}
}

@inproceedings{Heinen2011ACA,
  title={A connectionist approach for incremental function approximation and on-line tasks},
  author={Milton Roberto Heinen},
  year={2011}
}



@article{Ozturk2007AnalysisDesign,
author = {Ozturk, Mustafa C. and Xu, Dongming and Príncipe, José C.},
title = {Analysis and Design of Echo State Networks},
journal = {Neural Computation},
volume = {19},
number = {1},
pages = {111-138},
year = {2007},
doi = {10.1162/neco.2007.19.1.111},
URL = {https://doi.org/10.1162/neco.2007.19.1.111},
eprint = {https://doi.org/10.1162/neco.2007.19.1.111}
}


@article{JAEGER2007335,
title = {Optimization and applications of echo state networks with leaky- integrator neurons},
journal = {Neural Networks},
volume = {20},
number = {3},
pages = {335 - 352},
year = {2007},
note = {Echo State Networks and Liquid State Machines},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2007.04.016},
url = {http://www.sciencedirect.com/science/article/pii/S089360800700041X},
author = {Herbert Jaeger and Mantas Lukoševičius and Dan Popovici and Udo Siewert},
keywords = {Recurrent neural networks, Pattern generation, Speaker classification},
}

@article{Bengio1994, 
author={Y. {Bengio} and P. {Simard} and P. {Frasconi}}, 
journal={IEEE Transactions on Neural Networks}, 
title={Learning long-term dependencies with gradient descent is difficult}, 
year={1994}, 
volume={5}, 
number={2}, 
pages={157-166}, 
keywords={recurrent neural nets;learning (artificial intelligence);numerical analysis;long-term dependencies;gradient descent;recognition;production problems;prediction problems;recurrent neural network training;temporal contingencies;input/output sequence mapping;efficient learning;Recurrent neural networks;Production;Delay effects;Intelligent networks;Neural networks;Discrete transforms;Computer networks;Cost function;Neurofeedback;Displays}, 
doi={10.1109/72.279181}, 
ISSN={1941-0093}, 
month={March}
}

@article{Corsi2009,
author = {Corsi, Fulvio},
title = "{A Simple Approximate Long-Memory Model of Realized Volatility}",
journal = {Journal of Financial Econometrics},
volume = {7},
number = {2},
pages = {174-196},
year = {2009},
month = {02},
issn = {1479-8409},
doi = {10.1093/jjfinec/nbp001},
url = {https://doi.org/10.1093/jjfinec/nbp001},
eprint = {https://academic.oup.com/jfec/article-pdf/7/2/174/2543795/nbp001.pdf},
}


@article{Andersen2003ModelForecastVola,
author = {Andersen, Torben G. and Bollerslev, Tim and Diebold, Francis X. and Labys, Paul},
title = {Modeling and Forecasting Realized Volatility},
journal = {Econometrica},
volume = {71},
number = {2},
pages = {579-625},
keywords = {continuous–time methods, quadratic variation, realized volatility, high–frequency data, long memory, volatility forecasting, density forecasting, risk management},
doi = {10.1111/1468-0262.00418},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.00418},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1468-0262.00418},
abstract = {We provide a framework for integration of high–frequency intraday data into the measurement, modeling, and forecasting of daily and lower frequency return volatilities and return distributions. Building on the theory of continuous–time arbitrage–free price processes and the theory of quadratic variation, we develop formal links between realized volatility and the conditional covariance matrix. Next, using continuously recorded observations for the Deutschemark/Dollar and Yen/Dollar spot exchange rates, we find that forecasts from a simple long–memory Gaussian vector autoregression for the logarithmic daily realized volatilities perform admirably. Moreover, the vector autoregressive volatility forecast, coupled with a parametric lognormal–normal mixture distribution produces well–calibrated density forecasts of future returns, and correspondingly accurate quantile predictions. Our results hold promise for practical modeling and forecasting of the large covariance matrices relevant in asset pricing, asset allocation, and financial risk management applications.},
year = {2003}
}


@article{Andersen2001DistributionofVola,
author = {Torben G Andersen and Tim Bollerslev and Francis X Diebold and Paul Labys},
title = {The Distribution of Realized Exchange Rate Volatility},
journal = {Journal of the American Statistical Association},
volume = {96},
number = {453},
pages = {42-55},
year  = {2001},
publisher = {Taylor & Francis},
doi = {10.1198/016214501750332965},
URL = {https://doi.org/10.1198/016214501750332965},
eprint = {https://doi.org/10.1198/016214501750332965}
}



@book{Billingsley2008, 
author = {Billingsley, P.}, 
title = {Probabilty and measure}, 
year = {2008}, 
publisher = {John Wiley & Sons}, 
address = {USA} 
}

@article{NielsenShephard2004,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/3598838},
 author = {Ole E. Barndorff-Nielsen and Neil Shephard},
 journal = {Econometrica},
 number = {3},
 pages = {885--925},
 publisher = {[Wiley, Econometric Society]},
 title = {Econometric Analysis of Realized Covariation: High Frequency Based Covariance, Regression, and Correlation in Financial Economics},
 volume = {72},
 year = {2004}
}



@article{Andersen2011MMN,
title = "Realized volatility forecasting and market microstructure noise",
author = "Andersen, {Torben G.} and Tim Bollerslev and Nour Meddahi",
year = "2011",
month = "1",
day = "1",
doi = "10.1016/j.jeconom.2010.03.032",
language = "English (US)",
volume = "160",
pages = "220--234",
journal = "Journal of Econometrics",
issn = "0304-4076",
publisher = "Elsevier BV",
number = "1",
}


@article{Grigortega2016ParallelRC,
author = {Grigoryeva, Lyudmila and Henriques, Julie and Larger, Laurent and Ortega, Juan-Pablo},
title = {Nonlinear Memory Capacity of Parallel Time-Delay Reservoir Computers in the Processing of Multidimensional Signals},
journal = {Neural Computation},
volume = {28},
number = {7},
pages = {1411-1451},
year = {2016},
doi = {10.1162/NECO\_a\_00845},
note ={PMID: 27172266},
URL = {https://doi.org/10.1162/NECO_a_00845},
eprint = {https://doi.org/10.1162/NECO_a_00845}
}


@book{Kushner2003RLS, 
author = {Harold Kushner and George Yin}, 
title = {Stochastic Approximation and Recursive Algorithms and Applications}, 
year = {2003}, 
volume = {35},
isbn = {9780387217697}, 
publisher = {Springer-Verlag New York}, 
address = {USA}
}

@article{Kerridge1967ErrorsOfPrediction,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1266426},
 author = {D. Kerridge},
 journal = {Technometrics},
 number = {2},
 pages = {309--311},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Errors of Prediction in Multiple Regression with Stochastic Regressor Variables},
 volume = {9},
 year = {1967}
}

@article{LengensteinMaass2007EdgeOfChaos,
title = {Edge of chaos and prediction of computational performance for neural curcuit models},
author = {Robert Lengenstein, Wolfgang Maass},
journal = {Neural Networks},
number = {20},
volume = {3},
year = {2007}
}

@article{Buesing2010,
title = {Connectivity, dynamics and memory in reservoir computing with Binary and Analog Neurons},
year = {2010},
journal = {Neural Computation},
number = {22},
volume = {5},
author = {L. Büsing, B. Schrauwen, R. Lengenstein}
}


@article{Liu2019NaturalLogarithmRectifiedAF,
  title={Natural-Logarithm-Rectified Activation Function in Convolutional Neural Networks},
  author={Yang Liu and Jianpeng Zhang and Chao Gao and Jinghua Qu and Lixin Ji},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.03682}
}


@INPROCEEDINGS{KIM2001ARCSINHActivation, 
author={T. {Kim} and T. {Adali}}, 
booktitle={2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)}, 
title={Complex backpropagation neural network using elementary transcendental activation functions}, 
year={2001}, 
volume={2}, 
number={}, 
pages={1281-1284 vol.2}, 
keywords={backpropagation;transfer functions;neural nets;signal processing;nonlinear functions;inverse problems;modulation;neural network design;backpropagation neural network;transcendental activation functions;signal processing;uncoupled real sigmoidal functions;complex activation functions;magnitude distortion;QAM;hyperbolic activation function;circular activation function;nonconstant modulus modulated signals;TWT;inverse activation function;differentiable nonlinear activation function;phase distortion;hyperbolic tangent function;complex domain;bounded nonlinear activation function;Neural networks;Signal processing;Phase distortion;Backpropagation algorithms;Nonlinear distortion;Signal restoration;Phase modulation;Least squares approximation;Quadrature phase shift keying;Equations}, 
doi={10.1109/ICASSP.2001.941159}, 
ISSN={1520-6149}, 
month={May}
}


@article{Jaeger2005,
author = {Jaeger, Herbert},
year = {2002},
month = {01},
pages = {},
title = {Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the echo state network approach},
volume = {5},
journal = {GMD-Forschungszentrum Informationstechnik, 2002.}
}


@book{hastie01statisticallearning,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

@article{Lin2011StockTrading,
title = "Intelligent stock trading system based on improved technical analysis and Echo State Network",
journal = "Expert Systems with Applications",
volume = "38",
number = "9",
pages = "11347 - 11354",
year = "2011",
issn = "0957-4174",
doi = "https://doi.org/10.1016/j.eswa.2011.03.001",
url = "http://www.sciencedirect.com/science/article/pii/S095741741100409X",
author = "Xiaowei Lin and Zehong Yang and Yixu Song"
}

@article{Jarvis2010ExtendingStability,
AUTHOR={Jarvis, Sarah and Rotter, Stefan and Egert, Ulrich},   
TITLE={Extending Stability Through Hierarchical Clusters in Echo State Networks},      
JOURNAL={Frontiers in Neuroinformatics},      
VOLUME={4},     
PAGES={11},     
YEAR={2010},      
URL={https://www.frontiersin.org/article/10.3389/fninf.2010.00011},       
DOI={10.3389/fninf.2010.00011},      
ISSN={1662-5196}
}

@article{Verplancke2010Dialysis,
author = {Verplancke, Thierry and Looy, Stijn and Steurbaut, Kristof and Benoit, Dominique and Turck, Filip and De Moor, Georges and Decruyenaere, Johan},
year = {2010},
month = {01},
pages = {4},
title = {Novel time series analysis approach for prediction of dialysis in critically ill patients using echo-state networks},
volume = {10},
journal = {BMC medical informatics and decision making},
doi = {10.1186/1472-6947-10-4}
}

@article{Rigamoti2018UsefulLife,
title = "Ensemble of optimized echo state networks for remaining useful life prediction",
journal = "Neurocomputing",
volume = "281",
pages = "121 - 138",
year = "2018",
issn = "0925-2312",
doi = "https://doi.org/10.1016/j.neucom.2017.11.062",
url = "http://www.sciencedirect.com/science/article/pii/S0925231217318210",
author = "Marco Rigamonti and Piero Baraldi and Enrico Zio and Indranil Roychoudhury and Kai Goebel and Scott Poll"
}

@article{Rachez2014LanguageModelling,
author = {Rachez, A. and Hagiwara, M.},
year = {2014},
month = {12},
pages = {1969-1981},
title = {Language modeling using augmented echo state networks},
volume = {10},
journal = {International Journal of Innovative Computing, Information and Control}
}

@article{Markowitz1952,
author = {Markowitz, Harry},
title = {Portfolio Selection*},
journal = {The Journal of Finance},
volume = {7},
number = {1},
pages = {77-91},
doi = {10.1111/j.1540-6261.1952.tb01525.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-6261.1952.tb01525.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.1952.tb01525.x},
year = {1952}
}


@article{Fama1970,
 ISSN = {00221082, 15406261},
 URL = {http://www.jstor.org/stable/2325486},
 author = {Eugene F. Fama},
 journal = {The Journal of Finance},
 number = {2},
 pages = {383--417},
 publisher = {[American Finance Association, Wiley]},
 title = {Efficient Capital Markets: A Review of Theory and Empirical Work},
 volume = {25},
 year = {1970}
}


@article{Tanaka2019PhysicalRC,
title = "Recent advances in physical reservoir computing: A review",
journal = "Neural Networks",
volume = "115",
pages = "100 - 123",
year = "2019",
issn = "0893-6080",
doi = "https://doi.org/10.1016/j.neunet.2019.03.005",
url = "http://www.sciencedirect.com/science/article/pii/S0893608019300784",
author = "Gouhei Tanaka and Toshiyuki Yamane and Jean Benoit Héroux and Ryosho Nakane and Naoki Kanazawa and Seiji Takeda and Hidetoshi Numata and Daiju Nakano and Akira Hirose"
}

